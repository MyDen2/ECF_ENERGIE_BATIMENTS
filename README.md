# ECF DataPulse Analytics - Pipeline de DonnÃ©es Multi-Sources

## Titre Professionnel Data Engineer - RNCP35288
### CompÃ©tences Ã©valuÃ©es : C1.1, C1.3, C1.4

---

## Description du projet

Ce projet met en Å“uvre un pipeline de donnÃ©es complet pour lâ€™analyse des consommations Ã©nergÃ©tiques de bÃ¢timents, de lâ€™ingestion brute jusquâ€™Ã  la visualisation et aux recommandations dÃ©cisionnelles.

Lâ€™objectif est de dÃ©montrer la capacitÃ© Ã  :

Concevoir une architecture data robuste

Traiter des volumes significatifs avec Apache Spark

EnchaÃ®ner des traitements analytiques reproductibles

Restituer les rÃ©sultats sous forme claire et exploitable

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DonnÃ©es sources  â”‚
â”‚                  â”‚
â”‚ â€¢ Consommations  â”‚
â”‚ â€¢ BÃ¢timents      â”‚
â”‚ â€¢ DonnÃ©es mÃ©tÃ©o  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Traitement Spark         â”‚
â”‚                          â”‚
â”‚ â€¢ Nettoyage              â”‚
â”‚ â€¢ Normalisation          â”‚
â”‚ â€¢ AgrÃ©gations            â”‚
â”‚ â€¢ Parquet partitionnÃ©    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Analyse & Visualisation  â”‚
â”‚ (Pandas / Matplotlib /  â”‚
â”‚  Seaborn / Dashboard)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Restitution              â”‚
â”‚                          â”‚
â”‚ â€¢ Graphiques             â”‚
â”‚ â€¢ DÃ©tection dâ€™anomalies  â”‚
â”‚ â€¢ Recommandations        â”‚
â”‚ â€¢ Rapport & slides       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

## Structure du projet

```
ecf_energie/
â”œâ”€â”€ README.md                                # Instructions d'execution
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ batiments.csv
â”‚   â”œâ”€â”€ consommations_raw.csv
â”‚   â”œâ”€â”€ meteo_raw.csv
â”‚   â””â”€â”€ tarifs_energie.csv
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration_spark.ipynb
â”‚   â”œâ”€â”€ 02_nettoyage_spark.py
â”‚   â”œâ”€â”€ 03_agregations_spark.ipynb
â”‚   â”œâ”€â”€ 04_nettoyage_meteo_pandas.ipynb
â”‚   â”œâ”€â”€ 05_fusion_enrichissement.ipynb
â”‚   â”œâ”€â”€ 06_statistiques_descriptives.ipynb
â”‚   â”œâ”€â”€ 07_analyse_correlations.ipynb
â”‚   â”œâ”€â”€ 08_detection_anomalies.ipynb
â”‚   â”œâ”€â”€ 09_visualisations_matplotlib.ipynb
â”‚   â”œâ”€â”€ 10_visualisations_seaborn.ipynb
â”‚   â””â”€â”€ 11_dashboard_executif.ipynb
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ consommations_clean/               # Parquet partitionne
â”‚   â”œâ”€â”€ consommations_agregees.parquet
â”‚   â”œâ”€â”€ meteo_clean.csv
â”‚   â”œâ”€â”€ consommations_enrichies.csv
â”‚   â”œâ”€â”€ consommations_enrichies.parquet
â”‚   â”œâ”€â”€ matrice_correlation.csv
â”‚   â”œâ”€â”€ anomalies_detectees.csv
â”‚   â”œâ”€â”€ figures/                           # Tous les graphiques
â”‚   â””â”€â”€ rapport_synthese.md
â”œâ”€â”€ Dockerfile.pipeline
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

## Technologies utilisÃ©es

| Domaine              | Technologie             |
| -------------------- | ----------------------- |
| Traitement distribuÃ© | Apache Spark 3.5.3      |
| Analyse              | Pandas                  |
| Visualisation        | Matplotlib, Seaborn     |
| Orchestration        | Docker & Docker Compose |
| Formats              | CSV, Parquet            |
| Notebooks            | Jupyter / nbconvert     |


## Installation

### 1. DÃ©marrer l'infrastructure

```bash
# DÃ©marrer tous les services
docker-compose up -d

# VÃ©rifier l'Ã©tat des services
docker-compose ps
```

### 2. AccÃ¨s aux interfaces

| Service | URL | Identifiants |
|---------|-----|--------------|
| **Spark Master UI** | http://localhost:8080 |Supervision du cluster Spark (jobs workers, mÃ©moire, CPU) |
| **Spark Application UI** | http://localhost:4040 | DÃ©tails dâ€™exÃ©cution dâ€™un job Spark en cours |
| **JupyterLab** | http://localhost:8888 | Exploration interactive, analyses et visualisations |

âš ï¸ Le port 4040 est actif uniquement lorsquâ€™un job Spark est en cours dâ€™exÃ©cution.

AccÃ¨s JupyterLab

AccÃ¨s via navigateur :
ğŸ‘‰ http://localhost:8888

Les notebooks sont montÃ©s automatiquement depuis le dossier :
```bash
./notebooks
```

Les rÃ©sultats et fichiers gÃ©nÃ©rÃ©s sont disponibles dans :
```bash
./output
```

### 3. ExÃ©cuter le pipeline

```bash
# Pipeline complet
docker compose run --rm pipeline
```
Cette commande exÃ©cute automatiquement :

- Nettoyage Spark (02_nettoyage_spark.py)

- AgrÃ©gations Spark

- Analyses Pandas

- DÃ©tection dâ€™anomalies

- Visualisations

- Dashboard exÃ©cutif

- GÃ©nÃ©ration des livrables

## RÃ©sulats produits

- Parquet nettoyÃ© et partitionnÃ©

- Statistiques descriptives

- Analyse des corrÃ©lations

- DÃ©tection dâ€™anomalies Ã©nergÃ©tiques

- Graphiques haute rÃ©solution (PNG 300 dpi)

- Dashboard exÃ©cutif

- Rapport de synthÃ¨se

## Sources de donnÃ©es

Le projet sâ€™appuie sur des fichiers CSV structurÃ©s, reprÃ©sentant les consommations Ã©nergÃ©tiques de bÃ¢timents et leurs caractÃ©ristiques.

1ï¸âƒ£ Consommations Ã©nergÃ©tiques

Fichier : data_ecf/consommations_raw.csv

Contient les relevÃ©s de consommation Ã©nergÃ©tique par bÃ¢timent.

Colonne	Description
batiment_id	Identifiant unique du bÃ¢timent
timestamp	Date et heure du relevÃ©
type_energie	Type dâ€™Ã©nergie (Ã©lectricitÃ©, gaz, eau, etc.)
conso	Consommation brute mesurÃ©e
cout	CoÃ»t associÃ© Ã  la consommation

Ces donnÃ©es sont brutes, non nettoyÃ©es, et peuvent contenir :

- Valeurs aberrantes

- DonnÃ©es manquantes

- IncohÃ©rences temporelles

2ï¸âƒ£ RÃ©fÃ©rentiel bÃ¢timents

Fichier : data_ecf/batiments.csv

DÃ©crit les caractÃ©ristiques structurelles des bÃ¢timents.

Colonne	Description
batiment_id	Identifiant du bÃ¢timent
nom	Nom du bÃ¢timent
type	Type de bÃ¢timent (Ã©cole, mairie, logement, etc.)
commune	Commune dâ€™implantation
surface_m2	Surface en mÂ²
annee_construction	AnnÃ©e de construction
classe_energetique	Classe DPE (A Ã  G)
nb_occupants_moyen	Occupation moyenne

Ce fichier sert de rÃ©fÃ©rentiel de jointure pour enrichir les donnÃ©es de consommation.

3ï¸âƒ£ DonnÃ©es mÃ©tÃ©o (optionnelles)

Les donnÃ©es mÃ©tÃ©orologiques sont utilisÃ©es pour :

Analyser lâ€™impact de la tempÃ©rature sur la consommation

Identifier des anomalies non liÃ©es au comportement des bÃ¢timents

Elles sont traitÃ©es via Pandas dans les notebooks analytiques.

## Commandes utiles

```bash
# Logs du pipeline
docker-compose logs -f pipeline

# ArrÃªter l'infrastructure
docker-compose down

# Supprimer les donnÃ©es (volumes)
docker-compose down -v
```